{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining\n",
    "\n",
    "Text mining is the process of automatically extracting high-quality information from text. \n",
    "\n",
    "High-quality information is typically derived through the devising of patterns and trends through means such as statistical pattern learning.\n",
    "\n",
    "Typical text mining applications include:\n",
    "- Text categorization (or text classification),\n",
    "- Text clustering, \n",
    "- Sentiment analysis,\n",
    "- Named entity recognition, etc.\n",
    "\n",
    "# In this session\n",
    "1. Preprocessing: textual normalization, simple tokenization\n",
    "2. Stopword removal: its importance\n",
    "3. Verify Zipf Law with Oshumed medical abstract collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## How to use this notebook\n",
    "\n",
    "This environment is called [*Jupyter Notebook*](http://jupyter.org/).\n",
    "\n",
    "It has two types of *cells*:\n",
    "  * Markdown cells (like this one, where you can write notes)\n",
    "  * Code cells\n",
    "\n",
    "Run code cells by pressing **Shift+Enter**. Let's try..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run me: press Shift+Enter\n",
    "print(\"Hello, world!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is a hands on session, so time to time you have to write tits and bits of code. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code to print any string...\n",
    "\n",
    "# Then run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upper case, Punctuations\n",
    "A computer does not **require** upper case letters and punctuations. \n",
    "\n",
    "Note: Python already provides a list of punctuations. We simply need to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "s = \"Hello, World!!\"\n",
    "\n",
    "# To lower case\n",
    "s = s.lower()\n",
    "\n",
    "# Remove punctuations\n",
    "for punct in punctuation:\n",
    "    s = s.replace(punct, '')\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization : NLTK\n",
    "[Natural Language Toolkit (NLTK)](http://www.nltk.org/) is a platform to work with human or natural language data using Python.\n",
    "\n",
    "As usual, we will first convert everything to lowercase and remove punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw1 = \"Grenoble is a city in southeastern France, at the foot of the French Alps, on the banks of Isère.\"\n",
    "raw2 = \"Grenoble is the capital of the department of Isère and is an important scientific centre in France.\"\n",
    "\n",
    "# Write code here to convert everything in lower case and to remove punctuation.\n",
    "\n",
    "\n",
    "print(raw1)\n",
    "print(raw2)\n",
    "# Again, SHIFT+ENTER to run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK already provides us with modules to easily tokenize the text. We will tokenize pieces of raw texts using `word_tokenize` function of NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Tokenization using NLTK\n",
    "tokens1 = nltk.word_tokenize(raw1)\n",
    "tokens2 = nltk.word_tokenize(raw2)\n",
    "\n",
    "# print the tokens\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build a **NLTK Text** object to store tokenized texts. One or more text then can be merged to form a **TextCollection**. This provides many useful operations helpful to statistically analyze a collection of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build NLTK Text objects\n",
    "text1 = nltk.Text(tokens1)\n",
    "text2 = nltk.Text(tokens2)\n",
    "\n",
    "# A list of Text objects\n",
    "text_list = [text1, text2]\n",
    "\n",
    "# Build NLTK text collection\n",
    "text_collection = nltk.text.TextCollection(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK TextCollection object can be used to calculate basic statistics.\n",
    "  1. count the number of occurances (or term frequency) of a word\n",
    "  2. obtain a frequency distribution of all the words in the text\n",
    "\n",
    "Note: The NLTK Text objects created in the intermediate steps can also be used to calculate similar statistics at document level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Frequency of a word\n",
    "freq = text_collection.count(\"grenoble\")\n",
    "print(\"Frequency of word \\'grenoble\\' = \", freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Frequency distribution\n",
    "freq_dist = nltk.FreqDist(text_collection)\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's automate: write a function\n",
    "Using above steps, we will now write a function. We will call this function **raw_to_text**. This function will take a list of raw texts and will return a NLTK TextCollection objects, representing the list of input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts a list of raw text to a NLTK TextCollection object.\n",
    "Applies lower-casing and punctuation removal.\n",
    "Returns:\n",
    "text_collection - a NLTK TextCollection object\n",
    "\"\"\"\n",
    "def raw_to_text(raw_list):\n",
    "    text_list = []\n",
    "    for raw in raw_list:\n",
    "        # Write code for lower-casing and punctuation removal\n",
    "        \n",
    "        \n",
    "        # Write code to tokenize and create NLTK Text object\n",
    "        # Name the variable 'text' to store the Text object\n",
    "        \n",
    "        \n",
    "        # storing the text in the list\n",
    "        text_list.append(text) \n",
    "\n",
    "    # Write code to create TextCollection from the list text_list\n",
    "    text_collection = nltk.text.TextCollection(text_list) # TO DELETE\n",
    "    \n",
    "    # return text collection\n",
    "    return text_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function with some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_list_sample = [\"The dog sat on the mat.\", \"The cat sat on the mat!\", \"We have a mat in our house.\"]\n",
    "text_collection_sample = raw_to_text(raw_list_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before we can compute the frequency distribution for this collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code to compute the frequency 'mat' in the collection.\n",
    "freq = ...\n",
    "print(\"Frequency of word \\'mat\\' = \", freq)\n",
    "\n",
    "# Write code to compute and display the frequency distribution of text_collection_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Something bigger\n",
    "We will use [**DBPedia** Ontology Classification Dataset](https://drive.google.com/open?id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k). It includes first paragraphs of Wikipedia articles. Each paragraph is assigned one of **14 categories**. Here is an example of an abstract under *Written Work* catgory:\n",
    ">The Regime: Evil Advances/Before They Were Left Behind is the second prequel novel in the Left Behind series written by Tim LaHaye and Jerry B. Jenkins. It was released on Tuesday November 15 2005. This book covers more events leading up to the first novel Left Behind. It takes place from 9 years to 14 months before the Rapture.\n",
    "\n",
    "In this hands-on we will use 15,000 documents belonging to three categories, namely *Album*, *Film* and *Written Work*.\n",
    "\n",
    "The file **corpus.txt** supplied here, contains 15,000 documents. Each line of the file is a document.\n",
    "\n",
    "Now we will:\n",
    "   1. Load the documents as a list\n",
    "   2. Create a NLTK TextCollection\n",
    "   3. Analyze different counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading documents as a list\n",
    "raw_docs = open(\"corpus.txt\").read().splitlines()\n",
    "print(\"Loaded \" + str(len(raw_docs)) + \" documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code to create a NLTK TextCollection\n",
    "# Hint: use raw_to_text function\n",
    "text_collection = ...\n",
    "\n",
    "# Print total number of words in these documents\n",
    "print(\"Total number of words = \", len(text_collection))\n",
    "print(\"Total number of unique words = \", len(set(text_collection)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the freq distribution for this text collection of documents. Then let's see the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code to compute frequency distribution of text_collection\n",
    "freq_dist = ...\n",
    "\n",
    "# Let's see most common 10 words.\n",
    "freq_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Something does not seem right!!** Can you point out what?\n",
    "\n",
    "Let's try by visualizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing Python package for plotting \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To plot\n",
    "plt.subplots(figsize=(12,10))\n",
    "freq_dist.plot(30, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "   1. Just 30 most frequent tokens make up around 260,000 out of 709,460 ($\\approx 36.5\\%$)\n",
    "   2. Most of these are very common words such as articles, pronouns etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop word filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are words which are filtered out before or after processing of natural language data (text). There is no universal stop-word list. Often, stop word lists include short function words, such as \"the\", \"is\", \"at\", \"which\", and \"on\". Removing  stop-words has been shown to increase the performance of different tasks like search. \n",
    "\n",
    "A file of **stop_words.txt** is included. We will now:\n",
    "   1. Load the contents of the file 'stop_words.txt' where each line is a stop word, and create a stop-word list.\n",
    "   2. Modify the function **raw_to_text** to perform (a) stop-word removal (b) numeric words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load stop-word list from file 'stop_words.txt'\n",
    "stopwords = set(open('stop_words.txt').read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "VERSION 2\n",
    "Converts a list of raw text to a NLTK TextCollection object.\n",
    "Applies lower-casing, punctuation removal and stop-word removal.\n",
    "Returns:\n",
    "text_collection: a NLTK TextCollection object\n",
    "\"\"\"\n",
    "def raw_to_text_2(raw_list, stopwords):\n",
    "    text_list = []\n",
    "    for raw in raw_list:\n",
    "        # lower-casing and punctuation removal\n",
    "        raw = raw.lower()\n",
    "        for punct in punctuation: \n",
    "            raw = raw.replace(punct, '')\n",
    "        \n",
    "        # to tokenize\n",
    "        tokens = nltk.word_tokenize(raw)\n",
    "        \n",
    "        # --- NEW BLOCK ADDED\n",
    "        # to remove tokens which are stop-words\n",
    "        clean_tokens = []\n",
    "        for token in tokens:\n",
    "            if token not in stopwords and not token.isdigit():\n",
    "                clean_tokens.append(token)\n",
    "        \n",
    "        # build NLTK Text\n",
    "        text = nltk.Text(clean_tokens)\n",
    "        \n",
    "        # storing the text in the list\n",
    "        text_list.append(text) \n",
    "\n",
    "    # creating text collection\n",
    "    text_collection = nltk.text.TextCollection(text_list)\n",
    "    # return text collection\n",
    "    return text_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retest our small sample with the new version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_list_sample = [\"The dog sat on the mat.\", \"The cat sat on the mat!\", \"We have a mat in our house.\"]\n",
    "# Write code to obtain and see freq_dist_sample with the new raw-to-text_2\n",
    "# Note: raw_to_text_2 takes two inputs/arguments\n",
    "text_collection_sample = ...\n",
    "freq_dist_sample = ...\n",
    "freq_dist_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, rerun with the bigger document set and replot the cumulative word frequencies.\n",
    "\n",
    "Recall that we already have the documents loaded in the variable **raw_docs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code to create a NLTK TextCollection with raw_to_text_2\n",
    "text_collection = ...\n",
    "\n",
    "# Write code to compute frequency distribution of text_collection\n",
    "freq_dist = ...\n",
    "\n",
    "# Write code to plot the frequencies again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf law\n",
    "Verify whether the dataset follows the Zipf law, by plotting the data on a log-log graph, with the axes being log (rank order) and log (frequency). You expect to obtain an alomost straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "counts = np.array(list(freq_dist.values()))\n",
    "tokens = np.array(list(freq_dist.keys()))\n",
    "ranks = np.arange(1, len(freq_dist)+1)\n",
    "\n",
    "# Obtaining indices that would sort the array in descending order\n",
    "indices = np.argsort(-counts)\n",
    "frequencies = counts[indices]\n",
    "\n",
    "# Plotting the ranks vs frequencies\n",
    "plt.subplots(figsize=(12,10))\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Zipf plot for our data\")\n",
    "plt.xlabel(\"Frequency rank of token\")\n",
    "plt.ylabel(\"Absolute frequency of token\")\n",
    "plt.grid()\n",
    "plt.plot(ranks, frequencies, 'o', markersize=0.9)\n",
    "for n in list(np.logspace(-0.5, math.log10(len(counts)-1), 17).astype(int)):\n",
    "    dummy = plt.text(ranks[n], frequencies[n], \" \" + tokens[indices[n]],   \n",
    "                     verticalalignment=\"bottom\", horizontalalignment=\"left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
